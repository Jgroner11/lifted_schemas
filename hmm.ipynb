{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 5\n",
    "x_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init to uniform dist of shape (z_dim,)\n",
    "p_z_init = np.ones(z_dim) / z_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init to uniform dist of shape (z_dim, z_dim)\n",
    "p_latent_transition = np.ones((z_dim, z_dim)) / z_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init to uniform dist of shape (z_dim, x_dim)\n",
    "p_emit = np.ones((z_dim, x_dim)) / x_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.2 0.2 0.2 0.2] [[0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2]\n",
      " [0.2 0.2 0.2 0.2 0.2]] [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "print(p_z_init, p_latent_transition, p_emit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p_emit[z, 0] * p_z_init[z] for z in range(z_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([sum(p_emit[z, x] * p_z_init[z] for z in range(z_dim)) for x in range(x_dim)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# faster way with matrix multiply\n",
    "# p_z_init is shape (z_dim,)\n",
    "# p_emit is shape (z_dim, x_dim)\n",
    "# matrix multiply is the same as dot product. it takes (x, y) x (y, z) and returns (x, z)\n",
    "# dot product takes (y, ) and (y, z) and returns (z, )\n",
    "# in this case, it is (z_dim,) x (z_dim, x_dim) = (x_dim,)\n",
    "np.dot(p_z_init, p_emit) # p_z_init is (z_dim,) and p_emit is (z_dim, x_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0, 0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[0,0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2, 0.2, 0.2, 0.2, 0.2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(p_z_init, p_latent_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,1,2,3,1,2,3,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2 0.2 0.2 0.2 0.2]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "p_z = np.zeros((x.shape[0], z_dim))\n",
    "p_z[0] = p_z_init\n",
    "print(p_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "p_x = np.zeros((x.shape[0], x_dim))\n",
    "print(p_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FORWARD ALGORITHM\n",
    "for i in range(x.shape[0]):\n",
    "    # emission:\n",
    "    # calculate P(X_i = x) = sum_z P(X_i = x | Z_i = z) P(Z_i = z)\n",
    "    p_x[i] = np.dot(p_z[i], p_emit) # (x_dim, )\n",
    "\n",
    "    if i < x.shape[0] - 1:\n",
    "        # latent transition:\n",
    "        # calculate P(Z_{i+1} = z) = sum_z P(Z_{i+1} = z | Z_i = z) P(Z_i = z)\n",
    "        p_z[i+1] = np.dot(p_z[i], p_latent_transition) # (z_dim, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first calculate P(Z_i = z) for all i\n",
    "for i in range(x.shape[0] - 1):\n",
    "    p_z[i+1] = np.dot(p_z[i], p_latent_transition) # (z_dim, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then P(X_i = x) is just one big matrix multiply ish thing:\n",
    "# p_z is (seq_length, z_dim)\n",
    "# p_emit is (z_dim, x_dim)\n",
    "# output is (seq_length, x_dim)\n",
    "p_x = np.dot(p_z, p_emit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2, 0.2, 0.2, 0.2, 0.2],\n",
       "       [0.2, 0.2, 0.2, 0.2, 0.2],\n",
       "       [0.2, 0.2, 0.2, 0.2, 0.2],\n",
       "       [0.2, 0.2, 0.2, 0.2, 0.2],\n",
       "       [0.2, 0.2, 0.2, 0.2, 0.2],\n",
       "       [0.2, 0.2, 0.2, 0.2, 0.2],\n",
       "       [0.2, 0.2, 0.2, 0.2, 0.2],\n",
       "       [0.2, 0.2, 0.2, 0.2, 0.2],\n",
       "       [0.2, 0.2, 0.2, 0.2, 0.2],\n",
       "       [0.2, 0.2, 0.2, 0.2, 0.2],\n",
       "       [0.2, 0.2, 0.2, 0.2, 0.2],\n",
       "       [0.2, 0.2, 0.2, 0.2, 0.2]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25,\n",
       " 0.25000000000000006,\n",
       " 0.25000000000000006,\n",
       " 0.25000000000000006,\n",
       " 0.25000000000000006,\n",
       " 0.25000000000000006,\n",
       " 0.25000000000000006,\n",
       " 0.25000000000000006,\n",
       " 0.25000000000000006,\n",
       " 0.25000000000000006,\n",
       " 0.25000000000000006,\n",
       " 0.25000000000000006]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x is (seq_len, )\n",
    "# p_x is (seq_len, x_dim)\n",
    "[p_x[i, x[i]] for i in range(len(x))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
       "       0.25])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_x[range(len(x)), x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.960464477539077e-08"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(p_x[range(len(x)), x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 2]], which is output 0 of AsStridedBackward0, is at version 4; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mlog(p_x))\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Backpropagate\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 2]], which is output 0 of AsStridedBackward0, is at version 4; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define dimensions for our toy example\n",
    "x_dim = 3  # number of possible observations\n",
    "z_dim = 2  # number of possible latent states\n",
    "seq_length = 4  # length of sequence\n",
    "\n",
    "# Create random parameters with requires_grad=True\n",
    "p_emit = torch.rand((z_dim, x_dim), requires_grad=True)  # P(X|Z)\n",
    "p_latent_transition = torch.rand((z_dim, z_dim), requires_grad=True)  # P(Z_{t+1}|Z_t)\n",
    "initial_z = torch.rand(z_dim, requires_grad=True)  # P(Z_0)\n",
    "\n",
    "# Normalize probabilities using softmax\n",
    "p_emit = nn.functional.softmax(p_emit, dim=1)\n",
    "p_latent_transition = nn.functional.softmax(p_latent_transition, dim=1)\n",
    "initial_z = nn.functional.softmax(initial_z, dim=0)\n",
    "\n",
    "# Create a random sequence of observations (one-hot encoded)\n",
    "x = torch.randint(0, x_dim, (seq_length,))\n",
    "x_one_hot = torch.nn.functional.one_hot(x, x_dim).float()\n",
    "\n",
    "# Initialize arrays to store probabilities\n",
    "p_z = torch.zeros((seq_length, z_dim))\n",
    "p_x = torch.zeros(seq_length)\n",
    "\n",
    "# Initial state distribution\n",
    "p_z[0] = initial_z\n",
    "\n",
    "# Forward algorithm\n",
    "for i in range(seq_length):\n",
    "    # Emission probability: P(X_i = x | Z_i)\n",
    "    # Calculate P(X_i = x) = sum_z P(X_i = x | Z_i = z) P(Z_i = z)\n",
    "    emission_probs = torch.matmul(p_z[i], p_emit)  # (x_dim,)\n",
    "    p_x[i] = torch.sum(x_one_hot[i] * emission_probs)  # Probability of current observation\n",
    "\n",
    "    if i < seq_length - 1:\n",
    "        # Latent transition: P(Z_{i+1} | Z_i)\n",
    "        # Calculate P(Z_{i+1} = z) = sum_z P(Z_{i+1} = z | Z_i = z) P(Z_i = z)\n",
    "        p_z[i + 1] = torch.matmul(p_z[i], p_latent_transition)  # (z_dim,)\n",
    "\n",
    "# Calculate negative log-likelihood as loss\n",
    "loss = -torch.sum(torch.log(p_x))\n",
    "\n",
    "# Backpropagate\n",
    "loss.backward()\n",
    "\n",
    "# Print results\n",
    "print(f\"Observation sequence: {x}\")\n",
    "print(\"\\nEmission matrix P(X|Z):\")\n",
    "print(p_emit.detach())\n",
    "print(\"\\nTransition matrix P(Z_{t+1}|Z_t):\")\n",
    "print(p_latent_transition.detach())\n",
    "print(\"\\nInitial state distribution P(Z_0):\")\n",
    "print(initial_z.detach())\n",
    "print(\"\\nForward probabilities P(Z_t):\")\n",
    "print(p_z.detach())\n",
    "print(\"\\nObservation probabilities P(X_t):\")\n",
    "print(p_x.detach())\n",
    "print(\"\\nNegative log-likelihood:\", loss.item())\n",
    "\n",
    "# Print gradients\n",
    "print(\"\\nGradients:\")\n",
    "print(\"dL/dp_emit:\", p_emit.grad)\n",
    "print(\"dL/dp_latent_transition:\", p_latent_transition.grad)\n",
    "print(\"dL/dinitial_z:\", initial_z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Avg Loss: 5.1488\n",
      "Epoch 20, Avg Loss: 5.1263\n",
      "Epoch 30, Avg Loss: 5.1225\n",
      "Epoch 40, Avg Loss: 5.1208\n",
      "Epoch 50, Avg Loss: 5.1197\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ToyHMM(nn.Module):\n",
    "    def __init__(self, hidden_dim, obs_dim):\n",
    "        super().__init__()\n",
    "        self.log_init = nn.Parameter(torch.randn(hidden_dim))\n",
    "        self.log_trans = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.log_emit = nn.Parameter(torch.randn(hidden_dim, obs_dim))\n",
    "\n",
    "    def forward(self, obs_seq):\n",
    "        # obs_seq is shape (seq_len,)\n",
    "        seq_len = obs_seq.size(0)\n",
    "        # Convert parameters to probability distributions\n",
    "        init_probs = self.log_init.log_softmax(dim=0)          # (hidden_dim,)\n",
    "        trans_probs = self.log_trans.log_softmax(dim=1)        # (hidden_dim, hidden_dim)\n",
    "        emit_probs = self.log_emit.log_softmax(dim=1)          # (hidden_dim, obs_dim)\n",
    "\n",
    "        alpha = init_probs + emit_probs[:, obs_seq[0]]         # shape (hidden_dim,)\n",
    "        for t in range(1, seq_len):\n",
    "            # broadcast alpha to (hidden_dim, hidden_dim) by unsqueezing\n",
    "            alpha_next = torch.logsumexp(alpha.unsqueeze(1) + trans_probs, dim=0)\n",
    "            alpha = alpha_next + emit_probs[:, obs_seq[t]]\n",
    "\n",
    "        return torch.logsumexp(alpha, dim=0)  # log P(X)\n",
    "\n",
    "# Create a toy dataset (batch of sequences)\n",
    "# For simplicity, generate random observations in [0, obs_dim-1].\n",
    "hidden_dim, obs_dim = 2, 3\n",
    "seq_len, num_seqs = 5, 10\n",
    "data = torch.randint(low=0, high=obs_dim, size=(num_seqs, seq_len))\n",
    "\n",
    "model = ToyHMM(hidden_dim, obs_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    for seq in data:\n",
    "        optimizer.zero_grad()\n",
    "        log_prob = model(seq)  # forward algorithm\n",
    "        loss = -log_prob       # negative log-likelihood\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Avg Loss: {total_loss/num_seqs:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
